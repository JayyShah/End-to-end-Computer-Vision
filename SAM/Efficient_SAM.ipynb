{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "JXzgpRPcaq-d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KcYl4x_IagAx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Box and Point prompt"
      ],
      "metadata": {
        "id": "cbt7ZGa5ay1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ours_box_or_points(img_path, pts_sampled, pts_labels, model):\n",
        "    image_np = np.array(Image.open(img_path))\n",
        "    img_tensor = ToTensor()(image_np)\n",
        "    pts_sampled = torch.reshape(torch.tensor(pts_sampled), [1, 1, -1, 2])\n",
        "    pts_labels = torch.reshape(torch.tensor(pts_labels), [1, 1, -1])\n",
        "    predicted_logits, predicted_iou = model(\n",
        "        img_tensor[None, ...],\n",
        "        pts_sampled,\n",
        "        pts_labels,\n",
        "    )\n",
        "\n",
        "    sorted_ids = torch.argsort(predicted_iou, dim=-1, descending=True)\n",
        "    predicted_iou = torch.take_along_dim(predicted_iou, sorted_ids, dim=2)\n",
        "    predicted_logits = torch.take_along_dim(\n",
        "        predicted_logits, sorted_ids[..., None, None], dim=2\n",
        "    )\n",
        "\n",
        "    return torch.ge(predicted_logits[0, 0, 0, :, :], 0).cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "bXLt6kdeavjI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "JtRENQW6a282"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.8])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels == 1]\n",
        "    neg_points = coords[labels == 0]\n",
        "    ax.scatter(\n",
        "        pos_points[:, 0],\n",
        "        pos_points[:, 1],\n",
        "        color=\"green\",\n",
        "        marker=\"*\",\n",
        "        s=marker_size,\n",
        "        edgecolor=\"white\",\n",
        "        linewidth=1.25,\n",
        "    )\n",
        "    ax.scatter(\n",
        "        neg_points[:, 0],\n",
        "        neg_points[:, 1],\n",
        "        color=\"red\",\n",
        "        marker=\"*\",\n",
        "        s=marker_size,\n",
        "        edgecolor=\"white\",\n",
        "        linewidth=1.25,\n",
        "    )\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(\n",
        "        plt.Rectangle((x0, y0), w, h, edgecolor=\"yellow\", facecolor=(0, 0, 0, 0), lw=5)\n",
        "    )\n",
        "\n",
        "def show_anns_ours(mask, ax):\n",
        "    ax.set_autoscale_on(False)\n",
        "    img = np.ones((mask.shape[0], mask.shape[1], 4))\n",
        "    img[:, :, 3] = 0\n",
        "    color_mask = [0, 1, 0, 0.7]\n",
        "    img[np.logical_not(mask)] = color_mask\n",
        "    ax.imshow(img)\n"
      ],
      "metadata": {
        "id": "wFnSsTpLa1H5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the weights"
      ],
      "metadata": {
        "id": "RzU77nIpa9tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yformer/EfficientSAM.git\n",
        "import os\n",
        "os.chdir(\"EfficientSAM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvtXVqr0a7Qo",
        "outputId": "d9df6356-3e9c-4888-fa05-36586f5007db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'EfficientSAM' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from efficient_sam.build_efficient_sam import build_efficient_sam_vitt, build_efficient_sam_vits\n",
        "# from squeeze_sam.build_squeeze_sam import build_squeeze_sam\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "import zipfile\n",
        "models = {}\n",
        "\n",
        "# Build the EfficientSAM-Ti model.\n",
        "models['efficientsam_ti'] = build_efficient_sam_vitt()\n",
        "\n",
        "# Since EfficientSAM-S checkpoint file is >100MB, we store the zip file.\n",
        "with zipfile.ZipFile(\"weights/efficient_sam_vits.pt.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"weights\")\n",
        "# Build the EfficientSAM-S model.\n",
        "models['efficientsam_s'] = build_efficient_sam_vits()"
      ],
      "metadata": {
        "id": "A4_BaETeb9LF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load an image\n",
        "sample_image_np = np.array(Image.open(\"figs/examples/dogs.jpg\"))\n",
        "sample_image_tensor = transforms.ToTensor()(sample_image_np)\n",
        "# Feed a few (x,y) points in the mask as input.\n",
        "\n",
        "input_points = torch.tensor([[[[580, 350], [650, 350]]]])\n",
        "input_labels = torch.tensor([[[1, 1]]])\n",
        "\n",
        "# Run inference for both EfficientSAM-Ti and EfficientSAM-S models.\n",
        "for model_name, model in models.items():\n",
        "    print('Running inference using ', model_name)\n",
        "    predicted_logits, predicted_iou = model(\n",
        "        sample_image_tensor[None, ...],\n",
        "        input_points,\n",
        "        input_labels,\n",
        "    )\n",
        "    # The masks are already sorted by their predicted IOUs.\n",
        "    # The first dimension is the batch size (we have a single image. so it is 1).\n",
        "    # The second dimension is the number of masks we want to generate (in this case, it is only 1)\n",
        "    # The third dimension is the number of candidate masks output by the model.\n",
        "    # For this demo we use the first mask.\n",
        "    mask = torch.ge(predicted_logits[0, 0, 0, :, :], 0).cpu().detach().numpy()\n",
        "    masked_image_np = sample_image_np.copy().astype(np.uint8) * mask[:,:,None]\n",
        "    Image.fromarray(masked_image_np).save(f\"figs/examples/dogs_{model_name}_mask.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nq4_00_dL-Q",
        "outputId": "6604b283-3832-4373-ad14-020b2a2f7104"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference using  efficientsam_ti\n",
            "Running inference using  efficientsam_s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation"
      ],
      "metadata": {
        "id": "0I79d2BDbOtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1=400\n",
        "y1=200\n",
        "x2=800\n",
        "y2=600\n",
        "w=x2-x1\n",
        "h=y2-y1\n",
        "\n",
        "fig, ax = plt.subplots(1, 4, figsize=(30, 30))\n",
        "input_point = np.array([[x1, y1], [x2, y2]])\n",
        "input_label = np.array([2,3])\n",
        "image_path = \"figs/examples/dogs.jpg\"\n",
        "image = np.array(Image.open(image_path))\n",
        "show_points(input_point, input_label, ax[0])\n",
        "show_box([x1,y1,x2,y2], ax[0])\n",
        "ax[0].imshow(image)\n",
        "\n",
        "\n",
        "ax[1].imshow(image)\n",
        "mask_efficient_sam_vitt = run_ours_box_or_points(image_path, input_point, input_label, efficient_sam_vitt_model)\n",
        "show_anns_ours(mask_efficient_sam_vitt, ax[1])\n",
        "ax[1].title.set_text(\"EfficientSAM (VIT-tiny)\")\n",
        "ax[1].axis('off')\n",
        "\n",
        "ax[2].imshow(image)\n",
        "mask_efficient_sam_vits = run_ours_box_or_points(image_path, input_point, input_label, efficient_sam_vits_model)\n",
        "show_anns_ours(mask_efficient_sam_vits, ax[2])\n",
        "ax[2].title.set_text(\"EfficientSAM (VIT-small)\")\n",
        "ax[2].axis('off')\n",
        "\n",
        "\n",
        "ax[3].imshow(image)\n",
        "mask_squeeze_sam = run_ours_box_or_points(image_path, input_point, input_label, squeeze_sam_model)\n",
        "show_anns_ours(mask_squeeze_sam, ax[3])\n",
        "ax[3].title.set_text(\"SqueezeSAM\")\n",
        "ax[3].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SQK484AmbE0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Point Segmentation"
      ],
      "metadata": {
        "id": "m_Tf118RbT8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 4, figsize=(30, 30))\n",
        "\n",
        "input_label = np.array([1, 1])\n",
        "image_path = \"figs/examples/dogs.jpg\"\n",
        "input_point = np.array([[580, 350], [650, 350]])\n",
        "image = np.array(Image.open(image_path))\n",
        "show_points(input_point, input_label, ax[0])\n",
        "ax[0].imshow(image)\n",
        "\n",
        "\n",
        "ax[1].imshow(image)\n",
        "mask_efficient_sam_vitt = run_ours_box_or_points(image_path, input_point, input_label, efficient_sam_vitt_model)\n",
        "show_anns_ours(mask_efficient_sam_vitt, ax[1])\n",
        "ax[1].title.set_text(\"EfficientSAM (VIT-tiny)\")\n",
        "ax[1].axis('off')\n",
        "\n",
        "ax[2].imshow(image)\n",
        "mask_efficient_sam_vits = run_ours_box_or_points(image_path, input_point, input_label, efficient_sam_vits_model)\n",
        "show_anns_ours(mask_efficient_sam_vits, ax[2])\n",
        "ax[2].title.set_text(\"EfficientSAM (VIT-small)\")\n",
        "ax[2].axis('off')\n",
        "\n",
        "\n",
        "ax[3].imshow(image)\n",
        "mask_squeeze_sam = run_ours_box_or_points(image_path, input_point, input_label, squeeze_sam_model)\n",
        "show_anns_ours(mask_squeeze_sam, ax[3])\n",
        "ax[3].title.set_text(\"SqueezeSAM\")\n",
        "ax[3].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Auo6v9PybSJG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}