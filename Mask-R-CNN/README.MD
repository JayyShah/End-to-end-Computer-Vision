# Overview of Mask-RCNN

- Mask R-CNN is basically an extension of Faster R-CNN. Faster R-CNN is widely used for object detection tasks. For a given image, it returns the class label and bounding box coordinates for each object in the image.

- Letâ€™s first quickly understand how Faster R-CNN works. This will help us grasp the intuition behind Mask R-CNN as well.

    - Faster R-CNN first uses a **ConvNet** to extract feature maps from the images
    - These feature maps are then passed through a **Region Proposal Network (RPN)** which returns the candidate bounding boxes
    - We then apply an RoI pooling layer on these candidate bounding boxes to bring all the candidates to the same size
    - And finally, the proposals are passed to a fully connected layer to classify and output the bounding boxes for objects

## BackBone model

- Similar to the ConvNet that we use in Faster R-CNN to extract feature maps from the image, we use the ResNet 101 architecture to extract features from the images in Mask R-CNN. So, the first step is to take an image and extract features using the ResNet 101 architecture. These features act as an input for the next layer.

## Region Proposal Network (RPN)

- Taking the feature maps obtained in the previous step and apply a region proposal network (RPM). This basically predicts if an object is present in that region (or not). In this step, we get those regions or feature maps which the model predicts contain some object.

## Region of Interest (RoI)

- The regions obtained from the RPN might be of different shapes, right? Hence, we apply a pooling layer and convert all the regions to the same shape. Next, these regions are passed through a fully connected network so that the class label and bounding boxes are predicted.

Till this point, the steps are almost similar to how Faster R-CNN works. Now comes the difference between the two frameworks. In addition to this, Mask R-CNN also generates the segmentation mask.

For that, we first compute the region of interest so that the computation time can be reduced. For all the predicted regions, we compute the Intersection over Union (IoU) with the ground truth boxes. We can computer IoU like this:

IoU = Area of the intersection / Area of the union

Now, only if the IoU is greater than or equal to 0.5, we consider that as a region of interest. Otherwise, we neglect that particular region. We do this for all the regions and then select only a set of regions for which the IoU is greater than 0.5.


## Segmentation Mask

Once we have the RoIs based on the IoU values, we can add a mask branch to the existing architecture. This returns the segmentation mask for each region that contains an object. It returns a mask of size 28 X 28 for each region which is then scaled up for inference.


## Output:

- This will give us an array of 0s and 1s, where 0 means that there is no object at that particular pixel and 1 means that there is an object at that pixel. Note that the shape of the mask is similar to that of the original image

*Note: If the model has identified 3 objects in the image, the shape of the mask is (480, 640, 3). Had there been 5 objects, this shape would have been (480, 640, 5).* 